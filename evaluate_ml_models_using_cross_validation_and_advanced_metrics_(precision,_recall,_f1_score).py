# -*- coding: utf-8 -*-
"""Evaluate ML Models using Cross-Validation and Advanced Metrics (Precision, Recall, F1-score)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1639h-Ce0ARdLDqMdvKwWMzBDTe56wmSm
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_curve, auc
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------------
# Load your dataset
# -------------------------
df = pd.read_csv("/content/drive/MyDrive/saas_churn_data.csv")   # change file name

print(f"ðŸ“Š Dataset Shape: {df.shape}")
print(f"ðŸ”¹ Columns: {list(df.columns)}")
print(df.head())

# -------------------------
# Auto-detect target column (binary)
# -------------------------
binary_cols = [col for col in df.columns if df[col].nunique() == 2]
print(f"\nðŸ”Ž Binary column candidates found: {binary_cols}")
target_col = binary_cols[0] if binary_cols else df.columns[-1]
print(f"âœ… Selected target column: '{target_col}'")

X = df.drop(columns=[target_col])
y = df[target_col]

# Encode categorical features
for col in X.select_dtypes(include=['object']).columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# Standardize numeric features
X = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)

# -------------------------
# Models for comparison
# -------------------------
models = {
    "RandomForest": RandomForestClassifier(class_weight="balanced", random_state=42),
    "LogisticRegression": LogisticRegression(class_weight="balanced", max_iter=1000, random_state=42)
}

# -------------------------
# Stratified K-Fold Cross Validation
# -------------------------
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

results = {}
for name, model in models.items():
    print(f"\nðŸ”¹ Evaluating {name}...")
    acc_scores, prec_scores, rec_scores, f1_scores, auc_scores = [], [], [], [], []

    for train_idx, test_idx in skf.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:,1]

        # Metrics
        acc_scores.append(accuracy_score(y_test, y_pred))
        prec_scores.append(precision_score(y_test, y_pred))
        rec_scores.append(recall_score(y_test, y_pred))
        f1_scores.append(f1_score(y_test, y_pred))

        # ROC-AUC
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc_scores.append(auc(fpr, tpr))

    results[name] = {
        "Accuracy": (np.mean(acc_scores), np.std(acc_scores)),
        "Precision": (np.mean(prec_scores), np.std(prec_scores)),
        "Recall": (np.mean(rec_scores), np.std(rec_scores)),
        "F1": (np.mean(f1_scores), np.std(f1_scores)),
        "AUC": (np.mean(auc_scores), np.std(auc_scores))
    }

    # Print classification report (last fold)
    print("\nðŸ“Š Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["No Churn","Churn"], yticklabels=["No Churn","Churn"])
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {auc(fpr, tpr):.2f})")
    plt.plot([0,1],[0,1],'--',color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate (Recall)")
    plt.title(f"{name} - ROC Curve")
    plt.legend()
    plt.show()

# -------------------------
# Final Results Summary
# -------------------------
print("\nðŸ“Œ Cross-Validation Summary:")
for model, metrics in results.items():
    print(f"\nModel: {model}")
    for m, (mean, std) in metrics.items():
        print(f"{m}: {mean:.4f} Â± {std:.4f}")